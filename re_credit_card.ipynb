{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'pandas.core.series.Series'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import env\n",
    "\n",
    "env = gym.make('CREDITCARDFRAUD-v0')\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\conda\\envs\\yoloqt\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:133: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method should be an int or np.int64, actual type: <class 'pandas.core.series.Series'>\u001b[0m\n",
      "  logger.warn(f\"{pre} should be an int or np.int64, actual type: {type(obs)}\")\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    }
   ],
   "source": [
    "actions = []\n",
    "rewards = []\n",
    "count = 5\n",
    "\n",
    "while True:\n",
    "  action = env.action_space.sample()\n",
    "  reward = env.step(action)\n",
    "  actions.append(action)\n",
    "  rewards.append(reward)\n",
    "  \"\"\"\n",
    "  print(action)\n",
    "  print(reward)\n",
    "  \"\"\"\n",
    "  count -= 1\n",
    "  if count==0:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train_dataset_balanced.csv')\n",
    "df.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Load the datasets\n",
    "df_train = pd.read_csv('./data/train_dataset_balanced.csv')\n",
    "df_test = pd.read_csv('./data/test_dataset_balanced.csv')\n",
    "\n",
    "# Splitting the datasets into features and target variable\n",
    "X_train = df_train.iloc[:, :-1].values\n",
    "y_train = df_train['Class'].values\n",
    "X_test = df_test.iloc[:,: -1].values\n",
    "y_test = df_test['Class'].values\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train=X_train_scaled\n",
    "X_test=X_test_scaled \n",
    "Y_train=y_train\n",
    "Y_test=y_test\n",
    "\n",
    "X_train, X_test_, Y_train, Y_test_ = train_test_split(X_train, Y_train, test_size=0.5, random_state=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "X = df.iloc[:, :-1].values \n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(30, 16)\n",
    "        self.fc2 = nn.Linear(16, 18)\n",
    "        self.fc3 = nn.Linear(18, 20)\n",
    "        self.fc4 = nn.Linear(20, 24)\n",
    "        self.fc5 = nn.Linear(24, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.25)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN(\n",
      "  (fc1): Linear(in_features=30, out_features=16, bias=True)\n",
      "  (fc2): Linear(in_features=16, out_features=18, bias=True)\n",
      "  (fc3): Linear(in_features=18, out_features=20, bias=True)\n",
      "  (fc4): Linear(in_features=20, out_features=24, bias=True)\n",
      "  (fc5): Linear(in_features=24, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = DQN()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from collections import namedtuple, deque \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  #replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "EPSILON = 0.8           # probability of chosing on-policy action\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "    self.action_size = action_size\n",
    "    self.memory = deque(maxlen=buffer_size)\n",
    "    self.batch_size = batch_size\n",
    "    self.experiences = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    self.seed = random.seed(seed)\n",
    "  \n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    \"\"\"\n",
    "    print(f\"Adding state shape: {state.shape}, next_state shape: {next_state.shape}\")\n",
    "    \"\"\"\n",
    "    experience = self.experiences(state, action, reward, next_state, done)\n",
    "    self.memory.append(experience)\n",
    "  \"\"\"\n",
    "  def sample(self):\n",
    "     experiences = random.sample(self.memory, k=self.batch_size)\n",
    "     for i, e in enumerate(experiences):\n",
    "      if e is not None:\n",
    "          print('1________')\n",
    "          print(e)\n",
    "          print('2________')\n",
    "          print(f\"Index {i}: {e.state.shape}\")\n",
    "          print('3________')\n",
    "          print('')\n",
    "\n",
    "     states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).double().to(device)\n",
    "     actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).double().to(device)\n",
    "     rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).double().to(device)\n",
    "     next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).double().to(device)\n",
    "     dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).double().to(device)\n",
    "     return (states, actions, rewards, next_states, dones)\n",
    "  \"\"\"\n",
    "  def sample(self):\n",
    "    # 从经验池中随机采样一批数据\n",
    "    experiences = random.sample(self.memory, k=self.batch_size)\n",
    "    \n",
    "    # 初始化空列表来存储每个数据的对应部分\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    # 遍历采样到的每个经验\n",
    "    for i, e in enumerate(experiences):\n",
    "        # 检查是否有 None 类型的经验\n",
    "        if e is not None:\n",
    "            # 将各部分数据添加到对应的列表中\n",
    "            states.append(e.state)\n",
    "            actions.append(e.action)\n",
    "            rewards.append(e.reward)\n",
    "            next_states.append(e.next_state)\n",
    "            dones.append(e.done)\n",
    "            \"\"\"\n",
    "            print('1________')\n",
    "            print(e)\n",
    "            print('2________')\n",
    "            print(f\"Index {i}: {e.state.shape}\")\n",
    "            print('3________')\n",
    "            print('')\n",
    "            \"\"\"\n",
    "    # 将列表转换为 NumPy 数组，并转换为 PyTorch 张量，并移动到指定设备\n",
    "    states = torch.from_numpy(np.vstack(states)).double().to(device)\n",
    "    actions = torch.from_numpy(np.vstack(actions)).double().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack(rewards)).double().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack(next_states)).double().to(device)\n",
    "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).double().to(device)\n",
    "    \n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    # experiences = random.sample(self.memory, k=BATCH_SIZE)\n",
    "\n",
    "    # batch = self.experiences(*zip(experiences))\n",
    "\n",
    "    # states = torch.cat(batch.state)\n",
    "    # actions = torch.cat(batch.actions)\n",
    "    # rewards = torch.cat(batch.reward)\n",
    "    # next_states = torch.cat(batch.next_state)\n",
    "    # dones = torch.cat(batch.done)\n",
    "    #return random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "    \n",
    "  \n",
    "  def __len__(self):\n",
    "      return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "  def __init__(self, action_size, seed):\n",
    "    self.action_size = action_size\n",
    "    self.seed = random.seed(seed)\n",
    "\n",
    "\n",
    "    # Q - Network\n",
    "    self.qnet_local = DQN().double().to(device)\n",
    "    self.qnet_target = DQN().double().to(device)\n",
    "\n",
    "    self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=0.001)\n",
    "\n",
    "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "    self.t_step = 0\n",
    "    self.train_loss = []\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "    # learn every 4 timesteps\n",
    "    self.t_step = (self.t_step+1)%64\n",
    "    if self.t_step == 0:\n",
    "      experience = self.memory.sample()\n",
    "      #print('Experience sampled from memory : ', experience)\n",
    "      self.learn(experience, GAMMA)\n",
    "\n",
    "\n",
    "  def epsilon_greedy_action(self, state):\n",
    "    state = state.to(device)\n",
    "    self.qnet_local.eval()\n",
    "    with torch.no_grad():\n",
    "      action_values = self.qnet_local(state).max(1)[1]#.view(1, 1)\n",
    "    self.qnet_local.train()\n",
    "\n",
    "    if random.random() < 0.8:\n",
    "      print('Predicted action based on QNetwork : ', action_values)\n",
    "      return action_values.cpu()\n",
    "    else:\n",
    "      random_action = random.choices(np.arange(self.action_size), k=BATCH_SIZE)\n",
    "      print('Chosing  random actions for the batch : ', random_action)\n",
    "      return torch.DoubleTensor(random_action)\n",
    "  \n",
    "  def learn(self, experiences, gamma):\n",
    "    #print('Started learning')\n",
    "    states, actions, rewards, next_states, done = experiences#experiences[0].state, experiences[0].action, experiences[0].reward, experiences[0].next_state, experiences[0].done \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    self.qnet_local.train()\n",
    "    self.qnet_target.eval()\n",
    "\n",
    "    #predicted_targets = self.qnet_local(states)#.gather(1, actions)\n",
    "\n",
    "    #print(next_states.view(1, 1))\n",
    "    with torch.no_grad():\n",
    "      labels_next = self.qnet_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    \n",
    "    #print('labels_next {}'.format(labels_next))\n",
    "    \n",
    "    labels = 0 + (gamma * labels_next)\n",
    "    predicted_targets = self.qnet_local(states).gather(1, actions.long())\n",
    "\n",
    "    #print(\"Predicted targets : {}, labels : {}\".format(predicted_targets, labels))\n",
    "\n",
    "    loss = criterion(predicted_targets, labels).to(device)\n",
    "    print(\"===========================Training loss ============================\")\n",
    "    #print(loss.item())\n",
    "    self.train_loss.append(loss.item())\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    print('Total training losses : ', self.train_loss)\n",
    "\n",
    "    # perform soft update\n",
    "    self.soft_update(self.qnet_local, self.qnet_target, TAU)\n",
    "  \n",
    "  def soft_update(self, local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train).double()\n",
    "\n",
    "train = data_utils.TensorDataset(X_train, Y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m TPR \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m FPR \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 19\u001b[0m current_state \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m):  \n\u001b[0;32m     22\u001b[0m   \u001b[38;5;66;03m#print(\"==========================EPOCH {} COMPLETED===================\".format(i))  \u001b[39;00m\n\u001b[0;32m     24\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCurrent state : \u001b[39m\u001b[38;5;124m'\u001b[39m, i)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# check reward strategy once\n",
    "# add probability to epsilon_greedy\n",
    "import json\n",
    "deep_agent = Agent(action_size=2, seed=0)\n",
    "num_episodes = 1\n",
    "max_t = 1000\n",
    "state = 0\n",
    "env.state_idx = 0\n",
    "\n",
    "true_positive = []\n",
    "true_negative = []\n",
    "\n",
    "false_positive = []\n",
    "false_negative = []\n",
    "\n",
    "TPR = []\n",
    "FPR = []\n",
    "\n",
    "current_state = df.iloc[0, :-1].values\n",
    "\n",
    "for i in range(1):  \n",
    "  #print(\"==========================EPOCH {} COMPLETED===================\".format(i))  \n",
    "\n",
    "  print('Current state : ', i)\n",
    "  score = 0\n",
    "  for state_idx, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data    \n",
    "    action = deep_agent.epsilon_greedy_action(inputs)\n",
    "    for a in action:\n",
    "      next_state, reward, done, info = env.step(a)\n",
    "      deep_agent.step(current_state, a, reward, next_state, done)\n",
    "      current_state = next_state\n",
    "      roc_info = info  # if info is already a dictionary\n",
    "      \n",
    "      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 获取实际的训练损失数据长度\n",
    "epochs = len(deep_agent.train_loss)\n",
    "\n",
    "# 使用实际长度绘图\n",
    "plt.plot(range(epochs), deep_agent.train_loss, color='orange', label='Training Loss')\n",
    "\n",
    "# 添加图例\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# 标注坐标轴\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Training Loss')\n",
    "\n",
    "# 添加标题\n",
    "plt.title('Training Loss Across Epochs')\n",
    "\n",
    "# 显示图形\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.from_numpy(X_test)\n",
    "Y_test = torch.from_numpy(Y_test).double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = data_utils.TensorDataset(X_test, Y_test)\n",
    "test_loader = data_utils.DataLoader(test, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# 初始化用于存储预测和标签的列表\n",
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)  # 确保数据在模型所在的设备上\n",
    "        predicted = deep_agent.epsilon_greedy_action(inputs)\n",
    "        y_pred.extend(predicted.cpu().numpy())  # 保存预测结果\n",
    "        y_true.extend(labels.cpu().numpy())  # 保存真实标签\n",
    "\n",
    "# 计算混淆矩阵\n",
    "conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# 使用 classification_report 输出分类性能结果\n",
    "report = classification_report(y_true, y_pred, target_names=['not_fraud', 'fraud'])\n",
    "print(report)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloqt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
