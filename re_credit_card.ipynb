{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "C:\\Users\\hp\\AppData\\Roaming\\Python\\Python39\\site-packages\\gym\\utils\\passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'pandas.core.series.Series'>`\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "import env\n",
    "\n",
    "env = gym.make('CREDITCARDFRAUD-v0')\n",
    "\n",
    "obs = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if gpu is to be used\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "(V1          -0.110228\n",
      "V2          -0.055915\n",
      "V3           1.101883\n",
      "V4          -0.361325\n",
      "V5           0.330693\n",
      "V6           0.354809\n",
      "V7           0.820685\n",
      "V8          -0.183677\n",
      "V9           0.151746\n",
      "V10          0.796717\n",
      "V11         -0.091693\n",
      "V12          0.930089\n",
      "V13         -0.171066\n",
      "V14          0.960680\n",
      "V15          0.379954\n",
      "V16          0.457767\n",
      "V17          0.268488\n",
      "V18          0.424604\n",
      "V19          0.715268\n",
      "V20         -0.256254\n",
      "V21         -0.248458\n",
      "V22         -0.864470\n",
      "V23          0.114198\n",
      "V24         -0.102500\n",
      "V25         -1.226955\n",
      "V26         -2.503070\n",
      "V27         -0.899612\n",
      "V28         -0.995135\n",
      "Amount    6424.130000\n",
      "Class        0.000000\n",
      "Name: 246, dtype: float64, -1.0, False, {})\n",
      "[1, 1]\n",
      "(V1          -0.252653\n",
      "V2           0.143234\n",
      "V3           1.173496\n",
      "V4           0.216816\n",
      "V5           1.029062\n",
      "V6           0.693556\n",
      "V7           0.068887\n",
      "V8          -0.433364\n",
      "V9          -0.053598\n",
      "V10          1.777702\n",
      "V11         -0.399066\n",
      "V12          1.213165\n",
      "V13          1.300805\n",
      "V14          0.806959\n",
      "V15          0.600477\n",
      "V16          0.766250\n",
      "V17          0.265103\n",
      "V18          0.715232\n",
      "V19          0.917950\n",
      "V20          0.044805\n",
      "V21          0.385487\n",
      "V22         -1.121148\n",
      "V23         -1.640026\n",
      "V24         -1.316796\n",
      "V25         -1.291902\n",
      "V26         -0.097400\n",
      "V27          0.786461\n",
      "V28          0.874377\n",
      "Amount    9740.450000\n",
      "Class        0.000000\n",
      "Name: 247, dtype: float64, -1.0, False, {})\n",
      "[1, 1, 0]\n",
      "(V1            1.054595\n",
      "V2           -0.727940\n",
      "V3            1.207743\n",
      "V4           -0.714076\n",
      "V5           -0.228617\n",
      "V6            0.417328\n",
      "V7            0.093753\n",
      "V8           -0.135080\n",
      "V9            0.342106\n",
      "V10           0.927927\n",
      "V11          -1.426341\n",
      "V12           0.442678\n",
      "V13           0.743141\n",
      "V14           0.510388\n",
      "V15           1.842845\n",
      "V16           0.200595\n",
      "V17           0.263289\n",
      "V18           2.115400\n",
      "V19          -1.284738\n",
      "V20          -0.563434\n",
      "V21          -0.165560\n",
      "V22          -0.039648\n",
      "V23          -0.064447\n",
      "V24          -0.156645\n",
      "V25           0.128278\n",
      "V26           1.270083\n",
      "V27          -0.200872\n",
      "V28          -0.011320\n",
      "Amount    13863.760000\n",
      "Class         0.000000\n",
      "Name: 248, dtype: float64, 1.0, False, {})\n",
      "[1, 1, 0, 1]\n",
      "(V1           -0.784076\n",
      "V2           -2.645312\n",
      "V3            0.424183\n",
      "V4           -1.369288\n",
      "V5            1.715488\n",
      "V6            1.878343\n",
      "V7           -0.093927\n",
      "V8            0.270293\n",
      "V9            0.261640\n",
      "V10           0.313895\n",
      "V11          -1.592709\n",
      "V12           0.440504\n",
      "V13           0.174703\n",
      "V14           0.434988\n",
      "V15          -1.585139\n",
      "V16           1.472562\n",
      "V17           0.545276\n",
      "V18          -0.028917\n",
      "V19           0.230210\n",
      "V20           1.636023\n",
      "V21           0.203968\n",
      "V22           0.233860\n",
      "V23           0.423190\n",
      "V24           2.365106\n",
      "V25           0.932995\n",
      "V26          -0.420052\n",
      "V27          -0.302960\n",
      "V28          -1.394053\n",
      "Amount    19303.550000\n",
      "Class         0.000000\n",
      "Name: 249, dtype: float64, -1.0, False, {})\n",
      "[1, 1, 0, 1, 0]\n",
      "(V1            0.047962\n",
      "V2           -0.126723\n",
      "V3            1.165832\n",
      "V4           -0.843370\n",
      "V5            0.508507\n",
      "V6            0.535939\n",
      "V7            0.663008\n",
      "V8           -0.122290\n",
      "V9            0.146423\n",
      "V10           0.535668\n",
      "V11          -0.422900\n",
      "V12           0.969438\n",
      "V13           0.642459\n",
      "V14           0.808564\n",
      "V15           0.167774\n",
      "V16           0.997868\n",
      "V17           0.142579\n",
      "V18           0.665038\n",
      "V19           0.395765\n",
      "V20           0.005387\n",
      "V21          -0.204918\n",
      "V22          -0.626679\n",
      "V23          -0.213138\n",
      "V24          -1.399362\n",
      "V25          -0.174124\n",
      "V26           0.294207\n",
      "V27           0.078856\n",
      "V28           0.140468\n",
      "Amount    12573.410000\n",
      "Class         0.000000\n",
      "Name: 250, dtype: float64, 1.0, False, {})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "actions = []\n",
    "rewards = []\n",
    "count = 5\n",
    "\n",
    "while True:\n",
    "  action = env.action_space.sample()\n",
    "  reward = env.step(action)\n",
    "  actions.append(action)\n",
    "  rewards.append(reward)\n",
    "  print(actions)\n",
    "  print(reward)\n",
    "  count -= 1\n",
    "  if count==0:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 351 entries, 0 to 350\n",
      "Data columns (total 30 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   V1      351 non-null    float64\n",
      " 1   V2      351 non-null    float64\n",
      " 2   V3      351 non-null    float64\n",
      " 3   V4      351 non-null    float64\n",
      " 4   V5      351 non-null    float64\n",
      " 5   V6      351 non-null    float64\n",
      " 6   V7      351 non-null    float64\n",
      " 7   V8      351 non-null    float64\n",
      " 8   V9      351 non-null    float64\n",
      " 9   V10     351 non-null    float64\n",
      " 10  V11     351 non-null    float64\n",
      " 11  V12     351 non-null    float64\n",
      " 12  V13     351 non-null    float64\n",
      " 13  V14     351 non-null    float64\n",
      " 14  V15     351 non-null    float64\n",
      " 15  V16     351 non-null    float64\n",
      " 16  V17     351 non-null    float64\n",
      " 17  V18     351 non-null    float64\n",
      " 18  V19     351 non-null    float64\n",
      " 19  V20     351 non-null    float64\n",
      " 20  V21     351 non-null    float64\n",
      " 21  V22     351 non-null    float64\n",
      " 22  V23     351 non-null    float64\n",
      " 23  V24     351 non-null    float64\n",
      " 24  V25     351 non-null    float64\n",
      " 25  V26     351 non-null    float64\n",
      " 26  V27     351 non-null    float64\n",
      " 27  V28     351 non-null    float64\n",
      " 28  Amount  351 non-null    float64\n",
      " 29  Class   351 non-null    int64  \n",
      "dtypes: float64(29), int64(1)\n",
      "memory usage: 82.4 KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/train_dataset_1to100_1.csv')\n",
    "df.head() \n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = df.iloc[:, :-1].values \n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "sc = StandardScaler()\n",
    "X = sc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(31, 16)\n",
    "        self.fc2 = nn.Linear(16, 18)\n",
    "        self.fc3 = nn.Linear(18, 20)\n",
    "        self.fc4 = nn.Linear(20, 24)\n",
    "        self.fc5 = nn.Linear(24, 2)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.dropout(x, p=0.25)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "from collections import namedtuple, deque \n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  #replay buffer size\n",
    "BATCH_SIZE = 32         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "EPSILON = 0.8           # probability of chosing on-policy action\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "  def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "    self.action_size = action_size\n",
    "    self.memory = deque(maxlen=buffer_size)\n",
    "    self.batch_size = batch_size\n",
    "    self.experiences = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    self.seed = random.seed(seed)\n",
    "  \n",
    "  def add(self, state, action, reward, next_state, done):\n",
    "    experience = self.experiences(state, action, reward, next_state, done)\n",
    "    self.memory.append(experience)\n",
    "  \"\"\"\n",
    "  def sample(self):\n",
    "     experiences = random.sample(self.memory, k=self.batch_size)\n",
    "     for i, e in enumerate(experiences):\n",
    "      if e is not None:\n",
    "          print('1________')\n",
    "          print(e)\n",
    "          print('2________')\n",
    "          print(f\"Index {i}: {e.state.shape}\")\n",
    "          print('3________')\n",
    "          print('')\n",
    "\n",
    "     states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).double().to(device)\n",
    "     actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).double().to(device)\n",
    "     rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).double().to(device)\n",
    "     next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).double().to(device)\n",
    "     dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None])).double().to(device)\n",
    "     return (states, actions, rewards, next_states, dones)\n",
    "  \"\"\"\n",
    "  def sample(self):\n",
    "    # 从经验池中随机采样一批数据\n",
    "    experiences = random.sample(self.memory, k=self.batch_size)\n",
    "    \n",
    "    # 初始化空列表来存储每个数据的对应部分\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    next_states = []\n",
    "    dones = []\n",
    "    \n",
    "    # 遍历采样到的每个经验\n",
    "    for i, e in enumerate(experiences):\n",
    "        # 检查是否有 None 类型的经验\n",
    "        if e is not None:\n",
    "            # 将各部分数据添加到对应的列表中\n",
    "            states.append(e.state)\n",
    "            actions.append(e.action)\n",
    "            rewards.append(e.reward)\n",
    "            next_states.append(e.next_state)\n",
    "            dones.append(e.done)\n",
    "            print('1________')\n",
    "            print(e)\n",
    "            print('2________')\n",
    "            print(f\"Index {i}: {e.state.shape}\")\n",
    "            print('3________')\n",
    "            print('')\n",
    "    \n",
    "    # 将列表转换为 NumPy 数组，并转换为 PyTorch 张量，并移动到指定设备\n",
    "    states = torch.from_numpy(np.vstack(states)).double().to(device)\n",
    "    actions = torch.from_numpy(np.vstack(actions)).double().to(device)\n",
    "    rewards = torch.from_numpy(np.vstack(rewards)).double().to(device)\n",
    "    next_states = torch.from_numpy(np.vstack(next_states)).double().to(device)\n",
    "    dones = torch.from_numpy(np.vstack(dones).astype(np.uint8)).double().to(device)\n",
    "    \n",
    "    return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    # experiences = random.sample(self.memory, k=BATCH_SIZE)\n",
    "\n",
    "    # batch = self.experiences(*zip(experiences))\n",
    "\n",
    "    # states = torch.cat(batch.state)\n",
    "    # actions = torch.cat(batch.actions)\n",
    "    # rewards = torch.cat(batch.reward)\n",
    "    # next_states = torch.cat(batch.next_state)\n",
    "    # dones = torch.cat(batch.done)\n",
    "    #return random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "    \n",
    "  \n",
    "  def __len__(self):\n",
    "      return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "  def __init__(self, action_size, seed):\n",
    "    self.action_size = action_size\n",
    "    self.seed = random.seed(seed)\n",
    "\n",
    "\n",
    "    # Q - Network\n",
    "    self.qnet_local = DQN().double().to(device)\n",
    "    self.qnet_target = DQN().double().to(device)\n",
    "\n",
    "    self.optimizer = optim.Adam(self.qnet_local.parameters(), lr=0.001)\n",
    "\n",
    "    self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, seed)\n",
    "\n",
    "    self.t_step = 0\n",
    "    self.train_loss = []\n",
    "\n",
    "  def step(self, state, action, reward, next_state, done):\n",
    "    self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "\n",
    "    # learn every 4 timesteps\n",
    "    self.t_step = (self.t_step+1)%64\n",
    "    if self.t_step == 0:\n",
    "      experience = self.memory.sample()\n",
    "      #print('Experience sampled from memory : ', experience)\n",
    "      self.learn(experience, GAMMA)\n",
    "\n",
    "\n",
    "  def epsilon_greedy_action(self, state):\n",
    "    state = state.to(device)\n",
    "    self.qnet_local.eval()\n",
    "    with torch.no_grad():\n",
    "      action_values = self.qnet_local(state).max(1)[1]#.view(1, 1)\n",
    "    self.qnet_local.train()\n",
    "\n",
    "    if random.random() < 0.8:\n",
    "      print('Predicted action based on QNetwork : ', action_values)\n",
    "      return action_values.cpu()\n",
    "    else:\n",
    "      random_action = random.choices(np.arange(self.action_size), k=BATCH_SIZE)\n",
    "      print('Chosing  random actions for the batch : ', random_action)\n",
    "      return torch.DoubleTensor(random_action)\n",
    "  \n",
    "  def learn(self, experiences, gamma):\n",
    "    #print('Started learning')\n",
    "    states, actions, rewards, next_states, done = experiences#experiences[0].state, experiences[0].action, experiences[0].reward, experiences[0].next_state, experiences[0].done \n",
    "    criterion = torch.nn.BCELoss()\n",
    "    self.qnet_local.train()\n",
    "    self.qnet_target.eval()\n",
    "\n",
    "    #predicted_targets = self.qnet_local(states)#.gather(1, actions)\n",
    "\n",
    "    #print(next_states.view(1, 1))\n",
    "    with torch.no_grad():\n",
    "      labels_next = self.qnet_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "    \n",
    "    #print('labels_next {}'.format(labels_next))\n",
    "    \n",
    "    labels = 0 + (gamma * labels_next)\n",
    "    predicted_targets = self.qnet_local(states).gather(1, actions.long())\n",
    "\n",
    "    #print(\"Predicted targets : {}, labels : {}\".format(predicted_targets, labels))\n",
    "\n",
    "    loss = criterion(predicted_targets, labels).to(device)\n",
    "    print(\"===========================Training loss ============================\")\n",
    "    print(loss.item())\n",
    "    self.train_loss.append(loss.item())\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "\n",
    "    print('Total training losses : ', self.train_loss)\n",
    "\n",
    "    # perform soft update\n",
    "    self.soft_update(self.qnet_local, self.qnet_target, TAU)\n",
    "  \n",
    "  def soft_update(self, local_model, target_model, tau):\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "      target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train = torch.from_numpy(X_train)\n",
    "Y_train = torch.from_numpy(Y_train).double()\n",
    "\n",
    "train = data_utils.TensorDataset(X_train, Y_train)\n",
    "train_loader = data_utils.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current state :  0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasDgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state_idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader, \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m     27\u001b[0m   inputs, labels \u001b[38;5;241m=\u001b[39m data    \n\u001b[1;32m---> 28\u001b[0m   action \u001b[38;5;241m=\u001b[39m \u001b[43mdeep_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon_greedy_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m action:\n\u001b[0;32m     30\u001b[0m     next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n",
      "Cell \u001b[1;32mIn[43], line 34\u001b[0m, in \u001b[0;36mAgent.epsilon_greedy_action\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnet_local\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 34\u001b[0m   action_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqnet_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;66;03m#.view(1, 1)\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqnet_local\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m random\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.8\u001b[39m:\n",
      "File \u001b[1;32me:\\conda\\envs\\yoloqt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[48], line 10\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 10\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     11\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     12\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "File \u001b[1;32me:\\conda\\envs\\yoloqt\\lib\\site-packages\\torch\\nn\\modules\\module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1047\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32me:\\conda\\envs\\yoloqt\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\conda\\envs\\yoloqt\\lib\\site-packages\\torch\\nn\\functional.py:1847\u001b[0m, in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight):\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(linear, (\u001b[38;5;28minput\u001b[39m, weight), \u001b[38;5;28minput\u001b[39m, weight, bias\u001b[38;5;241m=\u001b[39mbias)\n\u001b[1;32m-> 1847\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INVALID_VALUE when calling `cublasDgemm( handle, opa, opb, m, n, k, &alpha, a, lda, b, ldb, &beta, c, ldc)`"
     ]
    }
   ],
   "source": [
    "# check reward strategy once\n",
    "# add probability to epsilon_greedy\n",
    "import json\n",
    "deep_agent = Agent(action_size=2, seed=0)\n",
    "num_episodes = 1\n",
    "max_t = 1000\n",
    "state = 0\n",
    "env.state_idx = 0\n",
    "\n",
    "true_positive = []\n",
    "true_negative = []\n",
    "\n",
    "false_positive = []\n",
    "false_negative = []\n",
    "\n",
    "TPR = []\n",
    "FPR = []\n",
    "\n",
    "current_state = df.iloc[0, :-1].values\n",
    "\n",
    "for i in range(1):  \n",
    "  #print(\"==========================EPOCH {} COMPLETED===================\".format(i))  \n",
    "\n",
    "  print('Current state : ', i)\n",
    "  score = 0\n",
    "  for state_idx, data in enumerate(train_loader, 0):\n",
    "    inputs, labels = data    \n",
    "    action = deep_agent.epsilon_greedy_action(inputs)\n",
    "    for a in action:\n",
    "      next_state, reward, done, info = env.step(a)\n",
    "      deep_agent.step(current_state, a, reward, next_state, done)\n",
    "      current_state = next_state\n",
    "      roc_info = info  # if info is already a dictionary\n",
    "      \n",
    "      \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(x, y1, \"-b\", label=\"sine\")\n",
    "\n",
    "\n",
    "plt.plot(range(4005), deep_agent.train_loss, color='orange', label='Training Loss')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.xlabel('#epochs')\n",
    "plt.ylabel('training loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoloqt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
